"""
main.py

The main entry point for the AI Advisor chat service, using Python and FastAPI.
This service powers the conversational AI interface, designed for extreme extensibility
and integration with a wide array of third-party services.
"""

import os
import logging
import uuid
from datetime import datetime
from typing import List, Optional, Dict, Any, AsyncGenerator

import uvicorn
from fastapi import (
    FastAPI,
    WebSocket,
    WebSocketDisconnect,
    Depends,
    HTTPException,
    status,
    Request,
    BackgroundTasks,
)
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from jose import JWTError, jwt
from pydantic import BaseModel, Field, BaseSettings
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

# --- Configuration Management ---
# In a real project, this would be in a separate `config.py` file.
class Settings(BaseSettings):
    """
    Manages application settings loaded from environment variables.
    """
    PROJECT_NAME: str = "AI Advisor Service"
    API_V1_STR: str = "/api/v1"
    DATABASE_URL: str = os.getenv("DATABASE_URL", "postgresql+asyncpg://user:password@localhost/ai_advisor")

    # Security & JWT
    SECRET_KEY: str = os.getenv("SECRET_KEY", "a_very_secret_key_for_dev")
    ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 7  # 7 days

    # Auth0 Integration
    AUTH0_DOMAIN: str = os.getenv("AUTH0_DOMAIN", "")
    AUTH0_API_AUDIENCE: str = os.getenv("AUTH0_API_AUDIENCE", "")

    # Core AI Provider (e.g., OpenAI, Anthropic, Google Gemini)
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")

    # Plaid Integration (Financial Data)
    PLAID_CLIENT_ID: str = os.getenv("PLAID_CLIENT_ID", "")
    PLAID_SECRET: str = os.getenv("PLAID_SECRET", "")
    PLAID_ENV: str = os.getenv("PLAID_ENV", "sandbox")

    # GCP Integration
    GCP_PROJECT_ID: str = os.getenv("GCP_PROJECT_ID", "")
    GCP_BUCKET_NAME: str = os.getenv("GCP_BUCKET_NAME", "ai-advisor-user-files")

    # AWS Integration
    AWS_ACCESS_KEY_ID: str = os.getenv("AWS_ACCESS_KEY_ID", "")
    AWS_SECRET_ACCESS_KEY: str = os.getenv("AWS_SECRET_ACCESS_KEY", "")
    AWS_REGION: str = os.getenv("AWS_REGION", "us-east-1")
    AWS_S3_BUCKET_NAME: str = os.getenv("AWS_S3_BUCKET_NAME", "ai-advisor-user-files-s3")

    class Config:
        case_sensitive = True
        env_file = ".env"

settings = Settings()

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Database Setup ---
# In a real project, this would be in a separate `database.py` file.
engine = create_async_engine(settings.DATABASE_URL, echo=False, future=True)
AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Dependency to get an async database session.
    """
    async with AsyncSessionLocal() as session:
        yield session

# --- Pydantic Schemas ---
# In a real project, these would be in a `schemas.py` file.
class Message(BaseModel):
    role: str = Field(..., description="Role of the message sender (e.g., 'user', 'assistant', 'system', 'tool')")
    content: str = Field(..., description="Content of the message")
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None

class ChatRequest(BaseModel):
    messages: List[Message]
    session_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    user_context: Optional[Dict[str, Any]] = None

class ChatResponse(BaseModel):
    message: Message
    session_id: str

class User(BaseModel):
    id: str
    email: Optional[str] = None
    # Add other user fields as needed from your user model/auth provider

class TokenData(BaseModel):
    sub: Optional[str] = None

# --- Authentication & Authorization ---
# In a real project, this would be in an `auth.py` or `security.py` file.
# This is a simplified example. For Auth0, you'd use a library like `python-jose`
# with JWKS to verify the token signature.
async def get_current_user(token: str, db: AsyncSession = Depends(get_db)) -> User:
    """
    Dependency to get the current user from a JWT token.
    This is a placeholder. A real implementation would:
    1. Fetch the JWKS from the Auth0 domain.
    2. Decode and validate the JWT token.
    3. Extract user information from the token payload.
    4. Potentially look up/create a user in the local database.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    if not token:
        raise credentials_exception
    try:
        # In a real Auth0 setup, you would use a more robust validation method
        # involving fetching keys from `https://{settings.AUTH0_DOMAIN}/.well-known/jwks.json`
        # For this example, we'll use a simplified local secret key for internal tokens.
        # payload = jwt.decode(token, settings.SECRET_KEY, algorithms=[settings.ALGORITHM])
        
        # Placeholder logic for demonstration
        if not token.startswith("Bearer "):
             raise credentials_exception
        user_id = token.split(" ")[1] # Fake user ID from token
        if user_id is None:
            raise credentials_exception
        token_data = TokenData(sub=user_id)
    except JWTError:
        raise credentials_exception
    
    # Here you would fetch the user from your database
    # user = await crud.user.get(db, id=token_data.sub)
    # if user is None:
    #     raise credentials_exception
    
    # Returning a mock user for now
    return User(id=token_data.sub, email=f"{token_data.sub}@example.com")


# --- Core AI Logic & Tool Integration ---
# This is the heart of the service. In a real project, this would be a complex
# module in `core/advisor.py` with submodules for each integration.
class AIAdvisor:
    """
    The core AI logic handler. It processes user messages, decides which tools
    to use (Plaid, GCP, AWS, etc.), and generates responses.
    """
    def __init__(self, user: User, db: AsyncSession):
        self.user = user
        self.db = db
        # Initialize clients for various services. These would be properly
        # configured and potentially injected.
        # self.llm_client = OpenAI(api_key=settings.OPENAI_API_KEY)
        # self.plaid_client = PlaidClient(...)
        # self.gcs_client = storage.Client(...)
        # self.s3_client = boto3.client('s3', ...)

    async def get_plaid_data(self, account_id: str) -> Dict:
        """Placeholder for fetching financial data from Plaid."""
        logger.info(f"TOOL_USE: Fetching Plaid data for user {self.user.id}, account {account_id}")
        # 1. Check if user has a Plaid access token in the DB.
        # 2. Use the token to call the Plaid API (e.g., transactions/get).
        # 3. Format the data for the LLM.
        return {"account_id": account_id, "balance": 12345.67, "currency": "USD"}

    async def analyze_gcs_document(self, file_path: str) -> str:
        """Placeholder for analyzing a document in Google Cloud Storage."""
        logger.info(f"TOOL_USE: Analyzing GCS document {file_path} for user {self.user.id}")
        # 1. Use GCS client to download the file.
        # 2. Process the file content (e.g., OCR, text extraction).
        # 3. Return a summary for the LLM.
        return f"Summary of document at {file_path}: The Q3 earnings were positive."

    async def list_s3_files(self, prefix: str) -> List[str]:
        """Placeholder for listing user files in an AWS S3 bucket."""
        logger.info(f"TOOL_USE: Listing S3 files with prefix '{prefix}' for user {self.user.id}")
        # 1. Use S3 client to list objects in the user's dedicated folder.
        return [f"{prefix}/report-2023.pdf", f"{prefix}/invoice-abc.docx"]

    async def stream_response(self, messages: List[Message]) -> AsyncGenerator[str, None]:
        """
        Processes messages and streams a response from the LLM, potentially using tools.
        """
        logger.info(f"Processing chat for user {self.user.id}")
        
        # This is a mock implementation. A real one would:
        # 1. Create a prompt for the LLM, including the message history and a description
        #    of available tools (Plaid, GCS, S3, etc.).
        # 2. Make a streaming call to the LLM API (e.g., OpenAI's ChatCompletion with stream=True).
        # 3. As chunks arrive:
        #    a. If it's a tool call, pause streaming, execute the tool function
        #       (e.g., `get_plaid_data`), and send the result back to the LLM in a new call.
        #    b. If it's a content chunk, yield it to the caller (WebSocket/HTTP endpoint).
        
        # Mock streaming response
        full_response = "Hello! As your AI Advisor, I can help with your finances, documents, and more. How can I assist you today?"
        for char in full_response:
            import asyncio
            await asyncio.sleep(0.02) # Simulate network latency and token generation
            yield char

# --- WebSocket Connection Manager ---
class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}

    async def connect(self, websocket: WebSocket, user_id: str):
        await websocket.accept()
        self.active_connections[user_id] = websocket

    def disconnect(self, user_id: str):
        if user_id in self.active_connections:
            del self.active_connections[user_id]

    async def send_personal_message(self, message: str, user_id: str):
        if user_id in self.active_connections:
            await self.active_connections[user_id].send_text(message)

    async def broadcast(self, message: str):
        for connection in self.active_connections.values():
            await connection.send_text(message)

manager = ConnectionManager()

# --- FastAPI Application Setup ---
app = FastAPI(
    title=settings.PROJECT_NAME,
    openapi_url=f"{settings.API_V1_STR}/openapi.json"
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, restrict this to your frontend domain(s)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- API Endpoints ---

@app.get("/health", tags=["Monitoring"])
async def health_check():
    """
    Simple health check endpoint.
    """
    return {"status": "ok", "timestamp": datetime.utcnow().isoformat()}

@app.post(f"{settings.API_V1_STR}/chat", response_model=ChatResponse, tags=["Chat"])
async def http_chat(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(lambda r: get_current_user(r.headers.get("Authorization", ""), Depends(get_db))),
    db: AsyncSession = Depends(get_db)
):
    """
    Standard HTTP endpoint for request-response chat interactions.
    """
    advisor = AIAdvisor(user=current_user, db=db)
    
    # For HTTP, we collect the full streamed response before returning.
    response_content = ""
    async for chunk in advisor.stream_response(request.messages):
        response_content += chunk

    # Log the interaction in the background
    # background_tasks.add_task(log_chat_interaction, db, current_user.id, request.session_id, request.messages, response_content)

    return ChatResponse(
        message=Message(role="assistant", content=response_content),
        session_id=request.session_id
    )

@app.websocket("/ws/v1/chat/{session_id}")
async def websocket_chat(
    websocket: WebSocket,
    session_id: str,
    token: str,
    db: AsyncSession = Depends(get_db)
):
    """
    WebSocket endpoint for real-time, streaming chat interactions.
    Authentication is performed via a token in the query parameters.
    """
    try:
        # We need to manually call the dependency with the token from the query param
        current_user = await get_current_user(f"Bearer {token}", db)
    except HTTPException:
        await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
        return

    await manager.connect(websocket, current_user.id)
    logger.info(f"WebSocket connected for user {current_user.id}, session {session_id}")
    
    advisor = AIAdvisor(user=current_user, db=db)
    
    try:
        while True:
            data = await websocket.receive_json()
            # Assuming the client sends data in the format of ChatRequest
            chat_request = ChatRequest(**data)
            
            # Stream the response back to the client chunk by chunk
            async for chunk in advisor.stream_response(chat_request.messages):
                await websocket.send_text(chunk)
            
            # Send an end-of-stream message if your protocol requires it
            # await websocket.send_json({"type": "end_of_stream"})

    except WebSocketDisconnect:
        manager.disconnect(current_user.id)
        logger.info(f"WebSocket disconnected for user {current_user.id}, session {session_id}")
    except Exception as e:
        logger.error(f"WebSocket error for user {current_user.id}: {e}")
        await websocket.close(code=status.WS_1011_INTERNAL_ERROR)
        manager.disconnect(current_user.id)


# --- Exception Handlers ---
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail},
    )

# --- Application Lifecycle Events ---
@app.on_event("startup")
async def startup_event():
    logger.info("AI Advisor Service is starting up...")
    # You can add initialization logic here, e.g., connecting to a cache like Redis.
    # await redis_client.ping()

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("AI Advisor Service is shutting down...")
    await engine.dispose()


if __name__ == "__main__":
    # This is for development purposes only.
    # In production, you would use a process manager like Gunicorn or Uvicorn directly.
    # Example: uvicorn src.services.ai-advisor-service.main:app --host 0.0.0.0 --port 8000 --reload
    uvicorn.run(app, host="0.0.0.0", port=8000)