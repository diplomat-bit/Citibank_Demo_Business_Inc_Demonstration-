# === Self-Hosted Runner for ML Model Training ===
# This workflow is specifically designed to run on a self-hosted runner.
# The 'runs-on' label targets a specific runner in your infrastructure
# that has been configured with GPUs and access to internal databases.

name: 'ML: Train Churn Model'

on:
  workflow_dispatch:

jobs:
  train-model:
    name: 'Train Customer Churn Model'
    # This label must match a label on your self-hosted runner
    runs-on: [self-hosted, linux, x64, gpu]
    timeout-minutes: 240 # 4 hours

    steps:
      - name: 'Checkout Repository'
        uses: actions/checkout@v4

      - name: 'Set up Python Environment'
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: 'Install Dependencies'
        run: |
          python -m pip install --upgrade pip
          pip install -r services/ml-trainer/requirements.txt

      - name: 'Check for GPU Access'
        run: nvidia-smi

      - name: 'Fetch Training Data from On-Premise Data Warehouse'
        env:
          # These secrets are stored in GitHub but the runner has network access
          # to the internal database.
          DW_HOST: ${{ secrets.ON_PREM_DW_HOST }}
          DW_USER: ${{ secrets.ON_PREM_DW_USER }}
          DW_PASSWORD: ${{ secrets.ON_PREM_DW_PASSWORD }}
        run: |
          echo "Connecting to internal data warehouse..."
          # Your script to pull data would go here
          python services/ml-trainer/fetch_data.py --output ./data/training.csv
          
      - name: 'Run Model Training'
        run: |
          echo "Starting training job on GPU..."
          python services/ml-trainer/train.py --data ./data/training.csv --output-model ./models/churn-model.pkl

      - name: 'Upload Model Artifact'
        uses: actions/upload-artifact@v4
        with:
          name: churn-model
          path: ./models/churn-model.pkl
