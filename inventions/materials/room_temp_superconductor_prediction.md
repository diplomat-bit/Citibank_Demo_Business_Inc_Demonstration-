# Room Temperature Superconductor Prediction using Graph Neural Networks

## Project Goal
Develop a Graph Neural Network (GNN) model capable of accurately predicting the critical temperature ($T_c$) of novel or known chemical compounds, with a specific focus on identifying candidates for room-temperature superconductivity ($T_c > 273.15 \text{ K}$).

## Methodology Overview

The problem of finding superconductors is fundamentally a materials science challenge that can be modeled as a predictive task on a graph structure.

1.  **Material Representation as a Graph:**
    *   **Nodes:** Individual atoms within the crystal structure.
    *   **Node Features ($\mathbf{h}_v$):** Atomic properties derived from the periodic table (e.g., atomic number, electronegativity, atomic radius, valence electron count).
    *   **Edges:** Chemical bonds or proximity interactions between atoms (determined by a cutoff distance).
    *   **Edge Features ($\mathbf{e}_{uv}$):** Bond characteristics (e.g., bond length, bond angle, interaction potential derived from density functional theory (DFT) approximations).

2.  **Graph Neural Network Architecture:**
    *   A message-passing framework (e.g., SchNet, DimeNet++, or a custom implementation using Graph Attention Networks - GAT) will be employed to iteratively update node embeddings by aggregating information from their neighbors.
    *   The final crystal embedding ($\mathbf{h}_{\text{crystal}}$) is obtained by pooling the final node embeddings (e.g., using a global mean or attention-based readout function).

3.  **Prediction Head:**
    *   The crystal embedding $\mathbf{h}_{\text{crystal}}$ is passed through a Multi-Layer Perceptron (MLP) regression head to output the predicted critical temperature, $\hat{T}_c$.

$$\hat{T}_c = \text{MLP}(\mathbf{h}_{\text{crystal}})$$

## Key Components & Implementation Details

### 1. Data Pipeline
*   **Dataset Sourcing:** Utilize curated databases like the Materials Project, SuperCon database, and specific literature compilations of high-$T_c$ hydrides and cuprates.
*   **Data Augmentation:** Generating slightly perturbed structures or modifying elemental substitutions to expand the training set.
*   **Target Variable:** $T_c$ (in Kelvin). Zero or very low $T_c$ values will be handled carefully, perhaps by using a logarithmic scale or treating them as baseline non-superconductors.

### 2. GNN Model Selection (Example: SchNet Adaptation)
We will adapt the structure-aware message passing inherent in SchNet, focusing on distance-based interactions, which are crucial for phonon-mediated superconductivity models (like the BCS theory approximation).

**Message Function:**
$$ \mathbf{m}_{uv}^{(k)} = \phi_e(\mathbf{e}_{uv}) \odot \phi_{dist}(||\mathbf{r}_u - \mathbf{r}_v||) \odot \mathbf{W}^{(k)} \cdot [\mathbf{h}_u^{(k-1)} || \mathbf{h}_v^{(k-1)}] $$

**Update Function (for node $u$):**
$$ \mathbf{h}_u^{(k)} = \mathbf{h}_u^{(k-1)} + \sum_{v \in \mathcal{N}(u)} \mathbf{m}_{uv}^{(k)} $$

Where $\phi_{dist}$ is typically implemented using continuous basis functions (like Gaussian radial basis functions) to capture interatomic distances effectively.

### 3. Training and Validation
*   **Loss Function:** Mean Squared Error (MSE) or Mean Absolute Error (MAE) on the predicted $T_c$.
    $$\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} (T_{c,i} - \hat{T}_c(G_i))^2$$
*   **Optimization:** Adam or RAdam optimizer with a cyclical learning rate schedule to navigate the complex energy landscape of materials space efficiently.
*   **Evaluation Metric:** Root Mean Squared Error (RMSE) and, critically, **Recall at Threshold $T_{target}$** (e.g., recall of all materials predicted to have $T_c > 270 \text{ K}$ that actually exceed $270 \text{ K}$).

## Deliverables and Next Steps

1.  **Trained GNN Model:** A checkpoint file containing the optimized weights.
2.  **Prediction Pipeline:** A script to ingest hypothetical crystal structures (e.g., defined by CIF files) and output the predicted $T_c$.
3.  **Candidate Prioritization:** Use the model to screen millions of hypothetical, stable crystal structures generated via generative chemistry models or high-throughput virtual screening, ranking them by predicted $T_c$.

**Invention Improvement Over Input:**
This leverages advanced deep learning on topological data (GNNs) which can capture complex, non-local interactions inherent in crystal structures far more effectively than traditional empirical or simple density functional theory (DFT) approximations used in older screening methods. It directly predicts the key property ($T_c$) rather than relying on intermediate proxies.