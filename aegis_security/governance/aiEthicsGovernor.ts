```typescript
/**
 * @file aegis_security/governance/aiEthicsGovernor.ts
 * @description Defines the AI Ethics Governor, a singleton agent responsible for monitoring all AI agent activities
 * to ensure strict compliance with the platform's constitutional and ethical frameworks. This governor acts as the
 * ultimate guardian of AI behavior within the system.
 */

import { v4 as uuidv4 } from 'uuid';

// --- Mock External Services and Data ---
// In a real application, these would be proper service clients and API calls.

/**
 * Simulates fetching constitutional articles from a data source.
 * This represents the foundational ethical and operational principles all AIs must adhere to.
 * @see ../../data/constitutionalArticles.ts
 */
async function fetchConstitutionalArticles(): Promise<ConstitutionalArticle[]> {
    console.log('[EthicsGovernor] Fetching constitutional framework...');
    // Simulating a network request
    await new Promise(resolve => setTimeout(resolve, 250));
    return [
        {
            id: 'article-001',
            name: 'Prohibition of Harmful Content',
            description: 'AI agents must not generate, promote, or distribute content that is hateful, violent, illegal, or harassing.',
            category: 'ContentSafety',
            severity: ViolationSeverity.CRITICAL,
            evaluationLogic: 'checkForHarmfulContent',
        },
        {
            id: 'article-002',
            name: 'Protection of Personal Data (PII)',
            description: 'AI agents must not leak, expose, or mishandle personally identifiable information (PII). All data must be anonymized where appropriate.',
            category: 'DataPrivacy',
            severity: ViolationSeverity.CRITICAL,
            evaluationLogic: 'checkForPII',
        },
        {
            id: 'article-003',
            name: 'Algorithmic Fairness and Bias Mitigation',
            description: 'AI agents must make decisions and generate content that is free from demographic, social, and cultural biases.',
            category: 'Fairness',
            severity: ViolationSeverity.HIGH,
            evaluationLogic: 'checkForBias',
        },
        {
            id: 'article-004',
            name: 'Truthfulness and Misinformation Prevention',
            description: 'AI agents must strive for accuracy and avoid the creation or propagation of misinformation or disinformation.',
            category: 'Integrity',
            severity: ViolationSeverity.HIGH,
            evaluationLogic: 'checkForMisinformation',
        },
        {
            id: 'article-005',
            name: 'Economic Fairness and Prohibition of Manipulation',
            description: 'Financial AI agents must provide advice that is in the best interest of the user, free from predatory tactics or market manipulation.',
            category: 'EconomicEthics',
            severity: ViolationSeverity.CRITICAL,
            evaluationLogic: 'checkForEconomicManipulation',
        },
        {
            id: 'article-006',
            name: 'Intellectual Property and Copyright Adherence',
            description: 'AI agents must respect intellectual property rights and avoid generating content that infringes on existing copyrights.',
            category: 'IP',
            severity: ViolationSeverity.MEDIUM,
            evaluationLogic: 'checkForIPInfringement',
        },
        {
            id: 'article-007',
            name: 'Secure and Safe Operations',
            description: 'AI agents must not perform actions that could compromise system security, such as generating insecure code or revealing internal system details.',
            category: 'Security',
            severity: ViolationSeverity.CRITICAL,
            evaluationLogic: 'checkForSecurityVulnerabilities',
        },
    ];
}

/**
 * Simulates an NLP analysis service, proxying to models like Gemini or GPT for complex analysis.
 * @see ../../api/gemini_openai_proxy_api.yaml
 */
const NLPAnalysisService = {
    analyzeText: async (text: string, analyses: ('bias' | 'pii' | 'harmful' | 'misinformation' | 'sentiment')[]) => {
        // In a real implementation, this would make an API call to a powerful NLP backend.
        // The backend would perform the requested analyses and return a structured result.
        console.log(`[NLPAnalysisService] Analyzing text for: ${analyses.join(', ')}`);
        await new Promise(resolve => setTimeout(resolve, 100)); // Simulate latency
        
        // Mock results based on keywords
        const result: Record<string, { detected: boolean; score: number; details: string }> = {};
        if (analyses.includes('harmful') && /hate|kill|violence/i.test(text)) {
            result.harmful = { detected: true, score: 0.95, details: 'Detected violent language.' };
        }
        if (analyses.includes('pii') && /\d{3}-\d{2}-\d{4}|ssn/i.test(text)) {
            result.pii = { detected: true, score: 0.99, details: 'Detected potential SSN.' };
        }
        if (analyses.includes('bias') && /should only hire men/i.test(text)) {
            result.bias = { detected: true, score: 0.88, details: 'Detected strong gender bias in hiring context.' };
        }
        if (analyses.includes('misinformation') && /earth is flat/i.test(text)) {
            result.misinformation = { detected: true, score: 0.92, details: 'Content contradicts established scientific fact.' };
        }

        return result;
    },
};

/**
 * Simulates a secure, immutable audit logging service.
 */
const AuditLogger = {
    log: async (event: ComplianceLog) => {
        console.log(`[AuditLogger] SECURE LOGGING: Governor event ${event.logId} for action ${event.actionId} processed. Status: ${event.status}`);
        // In a real system, this would write to a write-once, read-many (WORM) storage like AWS QLDB or a protected log stream.
        await new Promise(resolve => setTimeout(resolve, 50));
    },
};

/**
 * Simulates an AI Task Manager service that can control agent execution.
 * @see ../../components/services/ai/AITaskManagerService.ts
 */
const AITaskManager = {
    haltTask: async (taskId: string, reason: string): Promise<boolean> => {
        console.warn(`[AITaskManager] HALTING task ${taskId}. Reason: ${reason}`);
        // This would interact with the agent orchestration layer to stop a running process.
        return true;
    },
};

/**
 * Simulates a notification service for alerting human operators.
 * @see ../../components/notifications/AlertActionCenter.tsx
 */
const NotificationService = {
    createAlert: async (title: string, details: string, severity: ViolationSeverity): Promise<void> => {
        console.error(`[NotificationService] ALERT [${severity}]: ${title}. Details: ${details}`);
        // This would push a notification to a UI, send an email/SMS, or page an on-call engineer.
    },
};


// --- Core Types and Enums ---

export enum ComplianceStatus {
    COMPLIANT = 'COMPLIANT',
    NON_COMPLIANT = 'NON_COMPLIANT',
    UNDER_REVIEW = 'UNDER_REVIEW',
    ERRORED = 'ERRORED',
}

export enum ViolationSeverity {
    INFO = 'INFO',
    LOW = 'LOW',
    MEDIUM = 'MEDIUM',
    HIGH = 'HIGH',
    CRITICAL = 'CRITICAL',
}

export enum EnforcementActionType {
    LOG = 'LOG',
    ALERT_HUMAN = 'ALERT_HUMAN',
    REMEDIATE_OUTPUT = 'REMEDIATE_OUTPUT',
    HALT_AGENT_TASK = 'HALT_AGENT_TASK',
}

export interface AIAgent {
    id: string;
    name: string;
    version: string;
    type: 'TEXT_GENERATION' | 'DATA_ANALYSIS' | 'CODE_GENERATION' | 'FINANCIAL_ADVISOR';
}

export interface AIAgentAction {
    id: string; // Unique ID for this specific action
    agent: AIAgent;
    taskId: string; // The task this action is a part of
    timestamp: Date;
    input: {
        prompt?: string;
        data?: any;
        params?: Record<string, any>;
    };
    output: {
        response?: string;
        data?: any;
        decision?: any;
    };
    metadata?: Record<string, any>; // e.g., user session, IP address
}

export interface ConstitutionalArticle {
    id: string;
    name: string;
    description: string;
    category: 'ContentSafety' | 'DataPrivacy' | 'Fairness' | 'Integrity' | 'EconomicEthics' | 'IP' | 'Security';
    severity: ViolationSeverity;
    evaluationLogic: keyof AIEthicsGovernor['evaluationStrategies'];
}

export interface Violation {
    articleId: string;
    articleName: string;
    severity: ViolationSeverity;
    details: string;
    confidenceScore: number;
}

export interface ComplianceResult {
    status: ComplianceStatus;
    violations: Violation[];
}

export interface ComplianceLog extends ComplianceResult {
    logId: string;
    actionId: string;
    agentId: string;
    timestamp: Date;
    enforcementActions: EnforcementActionType[];
}

// --- The AI Ethics Governor ---

/**
 * @class AIEthicsGovernor
 * @singleton
 * @description A powerful AI agent that monitors, evaluates, and governs all other AI agents in the system.
 * It ensures that AI activities adhere to a predefined constitutional framework, mitigating risks related to bias,
 * privacy, security, and harmful content.
 */
export class AIEthicsGovernor {
    private static instance: AIEthicsGovernor;
    private constitutionalFramework: ConstitutionalArticle[] = [];
    private isInitialized = false;

    // A map of evaluation logic names to their implementation
    public readonly evaluationStrategies = {
        checkForHarmfulContent: this.evaluateForHarmfulContent.bind(this),
        checkForPII: this.evaluateForPII.bind(this),
        checkForBias: this.evaluateForBias.bind(this),
        checkForMisinformation: this.evaluateForMisinformation.bind(this),
        checkForEconomicManipulation: this.evaluateForEconomicManipulation.bind(this),
        checkForIPInfringement: this.evaluateForIPInfringement.bind(this),
        checkForSecurityVulnerabilities: this.evaluateForSecurityVulnerabilities.bind(this),
    };

    private constructor() {
        if (AIEthicsGovernor.instance) {
            throw new Error("AIEthicsGovernor is a singleton. Use AIEthicsGovernor.getInstance().");
        }
    }

    /**
     * Gets the singleton instance of the AIEthicsGovernor.
     */
    public static getInstance(): AIEthicsGovernor {
        if (!AIEthicsGovernor.instance) {
            AIEthicsGovernor.instance = new AIEthicsGovernor();
        }
        return AIEthicsGovernor.instance;
    }

    /**
     * Initializes the governor by loading the constitutional framework.
     * Must be called before monitoring can begin.
     */
    public async initialize(): Promise<void> {
        if (this.isInitialized) {
            console.log('[EthicsGovernor] Already initialized.');
            return;
        }
        console.log('[EthicsGovernor] Initializing...');
        this.constitutionalFramework = await fetchConstitutionalArticles();
        this.isInitialized = true;
        console.log(`[EthicsGovernor] Initialization complete. ${this.constitutionalFramework.length} articles loaded.`);
    }

    /**
     * The main entry point for monitoring an AI agent's action.
     * This method orchestrates the evaluation, enforcement, and logging process.
     * @param action The AIAgentAction to evaluate.
     * @returns A promise resolving to the final ComplianceLog.
     */
    public async monitor(action: AIAgentAction): Promise<ComplianceLog> {
        if (!this.isInitialized) {
            throw new Error('EthicsGovernor not initialized. Call initialize() first.');
        }

        const evaluationResult = await this.evaluate(action);
        const enforcementActions = await this.applyEnforcement(evaluationResult.violations, action);

        const log = this.createComplianceLog(action, evaluationResult, enforcementActions);
        await AuditLogger.log(log);

        // If the task was halted, we return a log indicating non-compliance.
        // The caller can use this log to decide whether to return the original output or an error message.
        return log;
    }

    /**
     * Evaluates an action against every article in the constitutional framework.
     * @param action The AIAgentAction to evaluate.
     * @returns A ComplianceResult summarizing the findings.
     */
    private async evaluate(action: AIAgentAction): Promise<ComplianceResult> {
        const allViolations: Violation[] = [];

        for (const article of this.constitutionalFramework) {
            const evaluationFn = this.evaluationStrategies[article.evaluationLogic];
            if (evaluationFn) {
                try {
                    const violation = await evaluationFn(action, article);
                    if (violation) {
                        allViolations.push(violation);
                    }
                } catch (error) {
                    console.error(`[EthicsGovernor] Error evaluating article ${article.id} for action ${action.id}:`, error);
                    // Optionally create a special violation for evaluation errors
                }
            } else {
                 console.warn(`[EthicsGovernor] No evaluation logic found for article ${article.id} ('${article.evaluationLogic}')`);
            }
        }

        return {
            status: allViolations.length > 0 ? ComplianceStatus.NON_COMPLIANT : ComplianceStatus.COMPLIANT,
            violations: allViolations,
        };
    }

    // --- Specific Evaluation Strategy Implementations ---

    private async evaluateForHarmfulContent(action: AIAgentAction, article: ConstitutionalArticle): Promise<Violation | null> {
        const textToScan = action.output.response || '';
        if (!textToScan) return null;

        const analysis = await NLPAnalysisService.analyzeText(textToScan, ['harmful']);
        if (analysis.harmful?.detected) {
            return {
                articleId: article.id,
                articleName: article.name,
                severity: article.severity,
                details: `Harmful content detected. Type: ${analysis.harmful.details}`,
                confidenceScore: analysis.harmful.score,
            };
        }
        return null;
    }

    private async evaluateForPII(action: AIAgentAction, article: ConstitutionalArticle): Promise<Violation | null> {
        const textToScan = action.output.response || JSON.stringify(action.output.data) || '';
        if (!textToScan) return null;

        const analysis = await NLPAnalysisService.analyzeText(textToScan, ['pii']);
        if (analysis.pii?.detected) {
            return {
                articleId: article.id,
                articleName: article.name,
                severity: article.severity,
                details: `PII leak detected. Details: ${analysis.pii.details}`,
                confidenceScore: analysis.pii.score,
            };
        }
        return null;
    }

    private async evaluateForBias(action: AIAgentAction, article: ConstitutionalArticle): Promise<Violation | null> {
        const textToScan = action.output.response || '';
        if (!textToScan) return null;

        const analysis = await NLPAnalysisService.analyzeText(textToScan, ['bias']);
        if (analysis.bias?.detected) {
            return {
                articleId: article.id,
                articleName: article.name,
                severity: article.severity,
                details: `Algorithmic bias detected. Details: ${analysis.bias.details}`,
                confidenceScore: analysis.bias.score,
            };
        }
        return null;
    }
    
    private async evaluateForMisinformation(action: AIAgentAction, article: ConstitutionalArticle): Promise<Violation | null> {
        const textToScan = action.output.response || '';
        if (!textToScan) return null;
        
        const analysis = await NLPAnalysisService.analyzeText(textToScan, ['misinformation']);
        if (analysis.misinformation?.detected) {
            return {
                articleId: article.id,
                articleName: article.name,
                severity: article.severity,
                details: `Potential misinformation detected. Claim: "${analysis.misinformation.details}"`,
                confidenceScore: analysis.misinformation.score,
            };
        }
        return null;
    }

    private async evaluateForEconomicManipulation(action: AIAgentAction, article: ConstitutionalArticle): Promise<Violation | null> {
        if (action.agent.type !== 'FINANCIAL_ADVISOR') return null;

        const decision = action.output.decision;
        if (!decision) return null;

        // Complex logic would go here. E.g., check if a recommended product has an unusually high fee,
        // or if risk assessments are downplayed for a high-commission product.
        if (decision.recommendation?.product === 'UltraHighRiskFund' && decision.userProfile?.riskTolerance === 'low') {
            return {
                articleId: article.id,
                articleName: article.name,
                severity: article.severity,
                details: `Potentially manipulative financial advice: Recommended high-risk product to low-tolerance user.`,
                confidenceScore: 0.9,
            };
        }
        return null;
    }

    private async evaluateForIPInfringement(action: AIAgentAction, article: ConstitutionalArticle): Promise<Violation | null> {
        const textToScan = action.output.response || '';
        if (!textToScan) return null;

        // This is a very complex problem. A real implementation might use a vector search against a database
        // of copyrighted works. For this mock, we'll use a simple keyword check.
        if (/\b(Harry Potter and the Chamber of Secrets|J.K. Rowling)\b/i.test(textToScan) && textToScan.length > 500) {
             return {
                articleId: article.id,
                articleName: article.name,
                severity: article.severity,
                details: `Content appears to contain significant portions of copyrighted text.`,
                confidenceScore: 0.75,
            };
        }
        return null;
    }
    
    private async evaluateForSecurityVulnerabilities(action: AIAgentAction, article: ConstitutionalArticle): Promise<Violation | null> {
        if (action.agent.type !== 'CODE_GENERATION') return null;
        
        const code = action.output.response;
        if (!code) return null;
        
        // Simple mock scanner for common vulnerabilities
        if (/\b(eval\(|dangerouslySetInnerHTML|password =|SECRET_KEY =)\b/i.test(code)) {
             return {
                articleId: article.id,
                articleName: article.name,
                severity: article.severity,
                details: `Generated code may contain a security vulnerability (e.g., use of 'eval', exposed credentials).`,
                confidenceScore: 0.85,
            };
        }
        // Check for SQL injection patterns
        if (/SELECT.*FROM.*WHERE.*' \+/.test(code)) {
             return {
                articleId: article.id,
                articleName: article.name,
                severity: article.severity,
                details: `Generated code appears to be vulnerable to SQL injection.`,
                confidenceScore: 0.95,
            };
        }
        return null;
    }

    /**
     * Applies enforcement actions based on the severity of detected violations.
     * @param violations An array of detected violations.
     * @param action The original action that caused the violations.
     */
    private async applyEnforcement(violations: Violation[], action: AIAgentAction): Promise<EnforcementActionType[]> {
        if (violations.length === 0) {
            return [EnforcementActionType.LOG];
        }

        const highestSeverity = violations.reduce((max, v) => 
            Object.values(ViolationSeverity).indexOf(v.severity) > Object.values(ViolationSeverity).indexOf(max) ? v.severity : max,
            ViolationSeverity.INFO
        );

        const enforcementActions: EnforcementActionType[] = [EnforcementActionType.LOG];
        const reason = `Ethics violation detected: ${violations.map(v => v.articleName).join(', ')}`;

        switch (highestSeverity) {
            case ViolationSeverity.CRITICAL:
                await AITaskManager.haltTask(action.taskId, reason);
                enforcementActions.push(EnforcementActionType.HALT_AGENT_TASK);
                // Fall-through to also alert
            case ViolationSeverity.HIGH:
                await NotificationService.createAlert(`High Severity AI Ethics Violation by ${action.agent.name}`, reason, highestSeverity);
                enforcementActions.push(EnforcementActionType.ALERT_HUMAN);
                break;
            case ViolationSeverity.MEDIUM:
                 await NotificationService.createAlert(`Medium Severity AI Ethics Violation by ${action.agent.name}`, reason, highestSeverity);
                enforcementActions.push(EnforcementActionType.ALERT_HUMAN);
                // Future: Could also trigger remediation
                break;
            case ViolationSeverity.LOW:
            case ViolationSeverity.INFO:
                // Only logging is performed for these levels, which is the default.
                break;
        }
        
        return enforcementActions;
    }

    /**
     * Creates a structured log entry for a compliance check.
     */
    private createComplianceLog(action: AIAgentAction, result: ComplianceResult, enforcementActions: EnforcementActionType[]): ComplianceLog {
        return {
            logId: `govlog-${uuidv4()}`,
            actionId: action.id,
            agentId: action.agent.id,
            timestamp: new Date(),
            status: result.status,
            violations: result.violations,
            enforcementActions,
        };
    }
}

/**
 * Export a singleton instance of the governor for use throughout the application.
 * The application's main entry point should call `ethicsGovernor.initialize()`
 * during its startup sequence.
 */
export const ethicsGovernor = AIEthicsGovernor.getInstance();
```